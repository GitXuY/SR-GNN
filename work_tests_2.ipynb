{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "source": "import tensorflow as tf\nimport numpy as np\nfrom functools import partial\nimport csv\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": "def data_generator(data):\n    for example in data:\n        yield example\n        ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": "def process_data(row):\n    features \u003d row[:-1]\n    labels \u003d row[-1]\n    items, alias_inputs \u003d tf.unique(features)\n\n    vector_length \u003d tf.shape(features)[0]\n    n_nodes \u003d tf.shape(items)[0]\n    indices \u003d tf.gather(alias_inputs, tf.stack([tf.range(vector_length - 1), tf.range(vector_length - 1) + 1],\n                                               axis\u003d0))  # Stack and stagger values\n    unique_indices, _ \u003d tf.unique(indices[0] * (vector_length + 1) + indices[1])  # unique(a*x + b)\n    unique_indices \u003d tf.sort(unique_indices)  # Sort ascending\n    unique_indices \u003d tf.stack(\n        [tf.floor_div(unique_indices, (vector_length + 1)), tf.floormod(unique_indices, (vector_length + 1))],\n        axis\u003d1)  # Ungroup and stack\n    unique_indices \u003d tf.cast(unique_indices, tf.int64)\n\n    values \u003d tf.ones(tf.shape(unique_indices, out_type\u003dtf.int64)[0], dtype\u003dtf.int64)\n    dense_shape \u003d tf.cast([n_nodes, n_nodes], tf.int64)\n\n    adj \u003d tf.SparseTensor(indices\u003dunique_indices, values\u003dvalues, dense_shape\u003ddense_shape)\n    adj \u003d tf.sparse.to_dense(adj)\n\n    u_sum_in_tf \u003d tf.math.reduce_sum(adj, 0)\n    u_sum_in_tf \u003d tf.clip_by_value(u_sum_in_tf, 1, tf.reduce_max(u_sum_in_tf))\n    A_in \u003d tf.math.divide(adj, u_sum_in_tf)\n\n    u_sum_out_tf \u003d tf.math.reduce_sum(adj, 1)\n    u_sum_out_tf \u003d tf.clip_by_value(u_sum_out_tf, 1, tf.reduce_max(u_sum_out_tf))\n    A_out \u003d tf.math.divide(tf.transpose(adj), u_sum_out_tf)\n\n    mask \u003d tf.fill(tf.shape(features), 1)\n\n    return A_in, A_out, alias_inputs, items, mask, labels\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": "def train_input_fn(batch_size):\n    with open(\"datasets/thg/processed/train.csv\", \"r\") as data_file:\n        data \u003d [list(map(int, rec)) for rec in csv.reader(data_file, delimiter\u003d\u0027,\u0027)]\n    max_seq \u003d len(max(data, key\u003dlen))\n    max_n_node \u003d len(max([np.unique(i) for i in data], key\u003dlen))\n\n    dataset \u003d tf.data.Dataset.from_generator(partial(data_generator, data), output_types\u003d(tf.int32))\n    dataset \u003d dataset.map(process_data)\n    dataset \u003d dataset.shuffle(100000)\n\n    dataset \u003d dataset.padded_batch(batch_size\u003dbatch_size, padded_shapes\u003d(\n        [max_n_node, max_n_node],\n        [max_n_node, max_n_node],\n        [max_seq],\n        [max_seq],\n        [max_seq],\n        []))\n\n    dataset \u003d dataset.prefetch(batch_size)\n    return dataset\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": "def eval_input_fn(batch_size):\n    with open(\"datasets/thg/processed/test.csv\", \"r\") as data_file:\n        data \u003d [list(map(int, rec)) for rec in csv.reader(data_file, delimiter\u003d\u0027,\u0027)]\n    max_seq \u003d len(max(data, key\u003dlen))\n    max_n_node \u003d len(max([np.unique(i) for i in data], key\u003dlen))\n\n    dataset \u003d tf.data.Dataset.from_generator(partial(data_generator, data), output_types\u003d(tf.int32))\n    dataset \u003d dataset.map(process_data)\n\n    dataset \u003d dataset.padded_batch(batch_size\u003dbatch_size, padded_shapes\u003d(\n        [max_n_node, max_n_node],\n        [max_n_node, max_n_node],\n        [max_seq],\n        [max_seq],\n        [max_seq],\n        []))\n\n    dataset \u003d dataset.prefetch(batch_size)\n    return dataset\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\nW0309 23:09:05.743708 4412249536 deprecation.py:323] From /Users/vladjiman/miniconda3/envs/SR-GNN/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:410: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\ntf.py_func is deprecated in TF V2. Instead, there are two\n    options available in V2.\n    - tf.py_function takes a python function which manipulates tf eager\n    tensors instead of numpy arrays. It\u0027s easy to convert a tf eager tensor to\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\n    means `tf.py_function`s can use accelerators such as GPUs as well as\n    being differentiable using a gradient tape.\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n    (it is not differentiable, and manipulates numpy arrays). It drops the\n    stateful argument making all functions stateful.\n    \n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "dataset \u003d train_input_fn(1)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": "\u003ctf.Tensor: id\u003d186, shape\u003d(3,), dtype\u003dint32, numpy\u003darray([ 1, 89, 89], dtype\u003dint32)\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 9
        }
      ],
      "source": "for i in dataset.take(1):\n    A_in, A_out, alias_inputs, items, mask, labels \u003d i\ntf.shape(A_in)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "source": "with open(\"datasets/thg/processed/train.csv\", \"r\") as data_file:\n    data \u003d [list(map(int, rec)) for rec in csv.reader(data_file, delimiter\u003d\u0027,\u0027)]",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "CPU times: user 4.44 s, sys: 85.6 ms, total: 4.53 s\nWall time: 4.5 s\n"
          ],
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "1183"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 28
        }
      ],
      "source": "%%time\nnp.amax([np.amax(z) for z in data])\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}